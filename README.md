# :camera_flash: Awesome-Project Homepage-Gallery

🚀🚀🚀 **An art gallery of Project Homepages aimed at providing inspiration for scientific articles and projects for researchers 💡. This repository will be continuously updated 🔥.**


## 📢 Latest Updates
- **Sep-23-2024**: 🎉 ${\color{red} The\ gallery\ is\ Created!}$

## Please share a $\color{orange} STAR$ ⭐ if this project does help

## 🧾 TODO List

- [ ] Provide toolkits :wrench: for creating project homepage. 
- [ ] Provide more inghts :bulb: for homepage designs.  

<!-- Tool part -->

## Tools
<details open>
<summary><b>Tools content</b></summary>

### 🎨 Color Palettes
* [ColorSpace](https://mycolor.space/)
* [Gradients](https://gradients.app/zh/gradient)
* [COOLORS](https://coolors.co/)
* [WebGradients](https://webgradients.com/)
* [UIColors](https://uicolors.app/create)
* [nipponcolors](https://nipponcolors.com/)
* [HappyHues](https://www.happyhues.co/)
* [LOLColors](https://www.webdesignrankings.com/resources/lolcolors/)

### 🎈 Icon
* [iconfont](https://www.iconfont.cn/)
* [Flaction](https://www.flaticon.com/icons)
* [Iconshock](https://www.iconshock.com/svg-color/)
* [IconFINDER](https://www.iconfinder.com/)
* [Dribbble](https://dribbble.com/tags/free_icons)

</details>

<details open>
<summary><b>HomePages</b></summary>

### 1. Visual Composer Object-level Visual Prompts for Compositional Image Generation
**Authors:** Gaurav Parmar, Or Patashnik, Kuan-Chieh Wang, Daniil Ostashev, Srinivasa Narasimhan, Jun-Yan Zhu, Daniel Cohen-Or, Kfir Aberman
<details span>
<summary><b>Abstract</b></summary>
We introduce a method for composing object-level visual prompts within a text-to-image diffusion model. Our approach addresses the task of generating semantically coherent compositions across diverse scenes and styles, similar to the versatility and expressiveness offered by text prompts. A key challenge in this task is to preserve the identity of the objects depicted in the input visual prompts, while also generating diverse compositions across different images. To address this challenge, we introduce a new KV-mixed cross-attention mechanism, in which keys and values are learned from distinct visual representations. The keys are derived from an encoder with a small bottleneck for layout control, whereas the values come from a larger bottleneck encoder that captures fine-grained appearance details. By mixing keys and values from these complementary sources, our model preserves identity of the visual prompts while supporting flexible variations in object arrangement, pose, and composition. During inference, we further propose object-level compositional guidance to improve the method's identity preservation and layout correctness. Results show that our technique produces diverse scene compositions that preserve the unique characteristics of each visual prompt, expanding the creative potential of text-to-image generation.
</details>

[📄 Paper](https://arxiv.org/abs/2501.01424) | [🌐 Project Page](https://snap-research.github.io/visual-composer/) | [💻 Code](https://snap-research.github.io/visual-composer/)

![Visual Composer](https://github.com/Ting-Devin-Han/awesome-project-homepage-gallery/raw/main/images/Visual%20Composer.jpg)
